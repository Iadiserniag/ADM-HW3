{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83562c6f",
   "metadata": {},
   "source": [
    "# Homework 3 - places of the world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb7bc5",
   "metadata": {},
   "source": [
    "**Name**: Giulia Iadisernia || **Matricola**: 2065450 || **Email**: iadisernia.2065450@studenti.uniroma1.it <br>\n",
    "**Name**: Stefano La Commare|| **Matricola**: 1919033 || **Email**: lacommare.1919033@studenti.uniroma1.it <br>\n",
    "**Name**: Francesco Ferrusi || **Matricola**: 2081332 || **Email**: francesco.ferrusi@outlook.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0be9f",
   "metadata": {},
   "source": [
    "## 1 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736d2cd",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a309af",
   "metadata": {},
   "source": [
    "####  Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6bac9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import operator\n",
    "from datetime import datetime\n",
    "import folium\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024f9b1",
   "metadata": {},
   "source": [
    "#### Get list of pages' URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50534c",
   "metadata": {},
   "source": [
    "* Every page's URL looks like `https://www.atlasobscura.com/places?page=i&sort=likes_count` for $1$ $\\leq$ $i$ $\\leq$ $400$\n",
    "* Therefore, we obtained a list of links (URLs) called `list_pages_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c5c71d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pages_url = []\n",
    "\n",
    "for i in range(1, 401):\n",
    "    if i == 1:\n",
    "        URL = 'https://www.atlasobscura.com/places?sort=likes_count'\n",
    "    else:\n",
    "        URL = 'https://www.atlasobscura.com/places?page=' + str(i) + '&sort=likes_count'\n",
    "    list_pages_url.append(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7acb7843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_pages_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98fb72",
   "metadata": {},
   "source": [
    "#### Get list of places for every page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ec0a2",
   "metadata": {},
   "source": [
    "* The URLs are composed by `https://www.atlasobscura.com` $+$ `the name of of the place` $+$ `?page=i&sort=likes_count` $for$ $1$ $\\leq$ $i$ $\\leq$ $400$.\n",
    "* We created a list containing all of the links named `complete_url_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6af8a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_url_list = []\n",
    "\n",
    "for i in range(0, len(list_pages_url)):\n",
    "    page = list_pages_url[i]\n",
    "    r = requests.get(page)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for a in soup.find_all('a', attrs = {'class': \"content-card content-card-place\"}):\n",
    "        if i+1 == 1: # first link\n",
    "            l = 'https://www.atlasobscura.com' + a['href'] + '?sort=likes_count'\n",
    "        else:\n",
    "            l = 'https://www.atlasobscura.com' + a['href'] + '?page=' + str(i+1) + '&sort=likes_count'\n",
    "        complete_url_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "74e5bdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_url_list) #7200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c0f9c",
   "metadata": {},
   "source": [
    "* Finally, we create an `output.txt` file that contains all of the links in the `complete_url_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5b264115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written successfully\n"
     ]
    }
   ],
   "source": [
    "# open file\n",
    "with open('output.txt', 'w+') as f:\n",
    "     \n",
    "    for items in complete_url_list:\n",
    "        f.write('%s\\n' %items)\n",
    "     \n",
    "    print(\"File written successfully\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4eec1",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c4f7b",
   "metadata": {},
   "source": [
    "* The function `save_links_as_html_files(txt_file, directory)` takes as inputs the `.txt` file containing the list of URLs, and the `directory` where we want to save our files.\n",
    "* It creates 400 folders named `Page_i` for $1$ $\\leq$ $i$ $\\leq$ $400$, where each folder contains 18 `.html` files named `Place_j.html` for for $1$ $\\leq$ $j$ $\\leq$ $7200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "f18789d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_links_as_html_files(txt_file, directory):\n",
    "    \n",
    "    with open(txt_file) as f:\n",
    "        files = [line.rstrip('\\n') for line in f]\n",
    "    f.close()\n",
    "    \n",
    "    folder_name = \"\"\n",
    "    folder_dir  = \"\"\n",
    "    file_name   = \"\"\n",
    "    file_dir    = \"\"\n",
    "    page_num    = 1\n",
    "\n",
    "    for m in range(0, 1800, 18):\n",
    "\n",
    "        folder_name = \"Page_\" + str(page_num)\n",
    "        folder_dir = directory + \"\\\\\" + folder_name\n",
    "\n",
    "        # create folder directory if it does not exist already\n",
    "        if not os.path.exists(folder_dir):\n",
    "            os.makedirs(folder_dir)\n",
    "\n",
    "        for i in range(0, 18):\n",
    "\n",
    "            file = files[i+m]\n",
    "            r = requests.get(file)\n",
    "\n",
    "            file_name = \"Place_\" + str(i+m+1) + \".html\"\n",
    "            file_dir = folder_dir + \"\\\\\" + file_name\n",
    "\n",
    "            with open(file_dir, \"wb+\") as f:\n",
    "                f.write(r.content)\n",
    "                \n",
    "            f.close()\n",
    "            \n",
    "        page_num += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "ea3324dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_links_as_html_files('output.txt', 'C:/Users\\iadig\\Desktop\\HW3_html_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53249305",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe535ae",
   "metadata": {},
   "source": [
    "* The function `parse_html_files(directory)` takes as input the `directory` of the folders containing the `.html` files outputted in point $1.2$.\n",
    "* It returns a list of parsed `.html` files. We used `BeautifulSoup()` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "76dbe956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_files(directory):\n",
    "\n",
    "    folder_name = \"\"\n",
    "    folder_dir  = \"\"\n",
    "    file_name   = \"\"\n",
    "    file_dir    = \"\"\n",
    "    page_num    = 1\n",
    "    List = []\n",
    "\n",
    "    for m in range(0, 1800, 18):\n",
    "\n",
    "        folder_name = \"Page_\" + str(page_num)\n",
    "        folder_dir = directory + \"\\\\\" + folder_name\n",
    "\n",
    "        for i in range(0, 18):\n",
    "\n",
    "            file_name = \"Place_\" + str(i+m+1) + \".html\"\n",
    "            file_dir = folder_dir + \"\\\\\" + file_name\n",
    "\n",
    "            with open(file_dir, \"rb\") as f:\n",
    "                soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "\n",
    "            f.close()\n",
    "              \n",
    "            List.append(soup)\n",
    "\n",
    "        page_num += 1\n",
    "    \n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "c0b97784",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_list = parse_html_files('C:/Users\\iadig\\Desktop\\HW3_html_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ceb96a",
   "metadata": {},
   "source": [
    "### The functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53620236",
   "metadata": {},
   "source": [
    "#### 1) Place Name (to save as placeName): String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "08efd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeName(soup_list):\n",
    "    \n",
    "    placeName_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        title = soup.find_all('h1', {'class':'DDPage__header-title'})\n",
    "        \n",
    "        if len(title) > 0: # there is a title\n",
    "            placeName = str(title[0].contents[0])\n",
    "            placeName_list.append(placeName)\n",
    "            \n",
    "        else:\n",
    "            placeName_list.append(float('NaN'))\n",
    "    \n",
    "    return placeName_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd9dd8",
   "metadata": {},
   "source": [
    "#### 2) Place Tags (to save as placeTags): List of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "a3de4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeTags(soup_list):\n",
    "    \n",
    "    placeTags_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        location = soup.find_all(\"div\", {'class': 'DDPage__header-place-location'})\n",
    "        \n",
    "        if len(location) > 0: # there is a title\n",
    "            loc = location[0].contents[0]\n",
    "            if type(loc) == Tag:\n",
    "                placeTags = str(loc.contents[0])\n",
    "            else:\n",
    "                placeTags = str(loc)\n",
    "                \n",
    "            placeTags_list.append(placeTags)\n",
    "            \n",
    "        else:\n",
    "            placeTags_list.append(float('NaN'))\n",
    "    \n",
    "    return placeTags_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b4aea",
   "metadata": {},
   "source": [
    "#### 3) No. of people who have been there (to save as numPeopleVisited): Integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "83c4d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_numPeopleVisited(soup_list):\n",
    "    \n",
    "    numPeopleVisited_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        num = soup.find_all('div', {'class': 'title-md item-action-count'})\n",
    "        \n",
    "        if len(num) > 0: # there is a title\n",
    "            numPeopleVisited = int(str(num[0].contents[0]))\n",
    "            numPeopleVisited_list.append(numPeopleVisited)\n",
    "            \n",
    "        else:\n",
    "            numPeopleVisited_list.append(float('NaN'))\n",
    "    \n",
    "    return numPeopleVisited_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2836172",
   "metadata": {},
   "source": [
    "#### 4) No. of people who want to visit the place(to save as numPeopleWant): Integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "d08a25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_numPeopleWant(soup_list):\n",
    "    \n",
    "    numPeopleWant_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        num = soup.find_all('div', {'class': 'title-md item-action-count'})\n",
    "        \n",
    "        if len(num) > 0: # there is a title\n",
    "            numPeopleWant = int(str(num[1].contents[0]))\n",
    "            numPeopleWant_list.append(numPeopleWant)\n",
    "            \n",
    "        else:\n",
    "            numPeopleWant_list.append(float('NaN'))\n",
    "    \n",
    "    return numPeopleWant_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fa1d1",
   "metadata": {},
   "source": [
    "#### 5) Description (to save as placeDesc): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "a0e3f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeDesc(soup_list):\n",
    "    \n",
    "    placeDesc_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        long_description = [x.text.strip() for x in soup.find_all('div', {'id': 'place-body'})]\n",
    "        \n",
    "        if len(long_description) > 0:\n",
    "            \n",
    "            placeDesc_list.append(long_description[0])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            placeDesc_list.append(float('NaN'))\n",
    "    \n",
    "    return placeDesc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7d28a",
   "metadata": {},
   "source": [
    "#### 6) Short Description (to save as placeShortDesc): String. Everything from the title and location up to the image (blue frame on the example image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "091f7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeShortDesc(soup_list):\n",
    "    \n",
    "    placeShortDesc_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        meta = soup.find_all('meta', {'property': \"og:description\"})\n",
    "        \n",
    "        if len(meta) > 0:\n",
    "            short_description = meta[0]['content'].rstrip()\n",
    "            \n",
    "            placeShortDesc_list.append(short_description)\n",
    "            \n",
    "        else:\n",
    "            placeShortDesc_list.append(float('NaN'))\n",
    "        \n",
    "\n",
    "    return placeShortDesc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cac3c5",
   "metadata": {},
   "source": [
    "#### 7) Nearby Places (to save as placeNearby): Extract the names of all nearby places, but only keep unique values: List of Strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "a71e7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeNearby(soup_list):\n",
    "    \n",
    "    placeNearby_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        placeNearby = []\n",
    "        \n",
    "        div = soup.find_all('div', class_=\"DDPageSiderailRecirc__item-title\")\n",
    "        \n",
    "        for i in range(0, len(div)):\n",
    "            placeNearby.append(str(div[i].contents[0]))\n",
    "            \n",
    "        if len(placeNearby) > 0:\n",
    "            placeNearby_list.append(placeNearby)\n",
    "        else:\n",
    "            placeNearby_list.append(float('NaN'))\n",
    "\n",
    "    return placeNearby_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d88d42",
   "metadata": {},
   "source": [
    "#### 8) Address of the place(to save as placeAddress): String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "41b6a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeAdress(soup_list):\n",
    "    \n",
    "    placeAdress_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        address = soup.find_all('address', {'class':\"DDPageSiderail__address\"})\n",
    "        \n",
    "        if len(address) > 0:\n",
    "        \n",
    "            placeAdress = \"\"\n",
    "            for el in address[0].find_all('div')[0].contents:\n",
    "                if type(el) == NavigableString:\n",
    "                    if placeAdress == \"\":\n",
    "                        placeAdress = el\n",
    "                    else:\n",
    "                        placeAdress = placeAdress + ' ' + el\n",
    "\n",
    "            placeAdress = placeAdress.rstrip(\"\\n\")\n",
    "            \n",
    "            placeAdress_list.append(placeAdress)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            placeAdress_list.append(float('NaN'))\n",
    "\n",
    "        \n",
    "    return placeAdress_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df896ae",
   "metadata": {},
   "source": [
    "#### 9) Altitude and Longitude of the place's location(to save as placeAlt and placeLong): Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e8bc11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeAlt_placeLong(soup_list):\n",
    "    \n",
    "    placeAlt_list = []\n",
    "    placeLong_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "    \n",
    "        div = soup.find_all(\"div\", {'class': 'DDPageSiderail__coordinates js-copy-coordinates'})\n",
    "\n",
    "        if len(div) > 0:\n",
    "            coordinates = div[0]['data-coordinates']\n",
    "\n",
    "            placeAlt, placeLong = coordinates.split(\", \")\n",
    "\n",
    "            placeAlt_list.append(float(placeAlt))\n",
    "            placeLong_list.append(float(placeLong))\n",
    "\n",
    "        else:\n",
    "            placeAlt_list.append(float('NaN'))\n",
    "            placeLong_list.append(float('NaN'))\n",
    "        \n",
    "    return [placeAlt_list, placeLong_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdfc4f",
   "metadata": {},
   "source": [
    "#### 10) The username of the post editors (to save as placeEditors): List of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "b519d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeEditors(soup_list):\n",
    "    \n",
    "    placeEditors_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "    \n",
    "        placeEditors = []\n",
    "        \n",
    "        contributors = soup.find_all(\"div\", {'class': 'DDPContributorsList'})\n",
    "\n",
    "        for el in contributors:\n",
    "            tag = el.find_all(\"span\")\n",
    "                \n",
    "        if len(tag) > 0:\n",
    "                    \n",
    "            for i in range(len(tag)):\n",
    "                placeEditors.append(str(tag[i].contents[0]))\n",
    "                \n",
    "            placeEditors_list.append(placeEditors)\n",
    "                \n",
    "        elif len(tag) == 0:\n",
    "                    \n",
    "            placeEditors_list.append(float('NaN'))\n",
    "            \n",
    "    return placeEditors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753766f5",
   "metadata": {},
   "source": [
    "#### 11) Post publishing date (to save as placePubDate): datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "60af8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placePubDate(soup_list):\n",
    "    \n",
    "    placePubDate_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        div = soup.find_all(\"div\", {'class': 'DDPContributor__name'})\n",
    "        \n",
    "        if len(div) > 0:\n",
    "            \n",
    "            placePubDate = str(div[0].contents[0])\n",
    "            placePubDate_list.append(placePubDate)\n",
    "                            \n",
    "        else:\n",
    "            \n",
    "            placePubDate_list.append(float('NaN'))\n",
    "        \n",
    "    return placePubDate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168557d1",
   "metadata": {},
   "source": [
    "#### 12) The names of the lists that the place was included in (to save as placeRelatedLists): List of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "f7ad4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeRelatedLists(soup_list):\n",
    "    \n",
    "    placeRelated_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        placeRelatedLists = []\n",
    "        \n",
    "        external_div = soup.find_all('div', class_=\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\")\n",
    "        \n",
    "        if len(external_div) > 2:\n",
    "            \n",
    "            internal_h3 = external_div[2].find_all('h3', class_=\"Card__heading --content-card-v2-title js-title-content\")\n",
    "            \n",
    "            for place_list in internal_h3:\n",
    "                placeRelatedLists.append(str(place_list.find_all(\"span\")[0].contents[0]))\n",
    "                 \n",
    "            placeRelated_list.append(placeRelatedLists)\n",
    "                \n",
    "        else:\n",
    "            placeRelated_list.append(float('NaN'))\n",
    "            \n",
    "\n",
    "    return placeRelated_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7eb27",
   "metadata": {},
   "source": [
    "#### 13) The names of the related places (to save as placeRelatedPlaces): List of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "12eb17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeRelatedPlaces(soup_list):\n",
    "    \n",
    "    placeRelatedPlaces_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        placeRelatedPlaces = []\n",
    "        \n",
    "        external_div = soup.find_all('div', class_=\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\")\n",
    "        \n",
    "        if len(external_div) > 1:\n",
    "            \n",
    "            internal_h3 = external_div[1].find_all('h3', class_=\"Card__heading --content-card-v2-title js-title-content\")\n",
    "            \n",
    "            for place_list in internal_h3:\n",
    "                placeRelatedPlaces.append(str(place_list.find_all(\"span\")[0].contents[0]))\n",
    "                \n",
    "            placeRelatedPlaces_list.append(placeRelatedPlaces)\n",
    "                \n",
    "        else:\n",
    "            placeRelatedPlaces_list.append(float('NaN'))\n",
    "            \n",
    "\n",
    "    return placeRelatedPlaces_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc252a",
   "metadata": {},
   "source": [
    "#### 14) The URL of the page of the place (to save as placeURL):String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b520f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeURL(soup_list):\n",
    "    \n",
    "    placeURL_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        \n",
    "        link = soup.find_all(\"link\", {'rel': 'canonical'})\n",
    "        \n",
    "        if len(link) > 0:\n",
    "            \n",
    "            placeURL = str(link[0]['href'])\n",
    "            \n",
    "            placeURL_list.append(placeURL)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            placeURL_list.append(float('NaN'))\n",
    "        \n",
    "    return placeURL_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f467ae",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4950f20",
   "metadata": {},
   "source": [
    "* Now, we call the `functions` to extract the places' information.\n",
    "* If the some of the information that we want is not found, we simply insert a `NaN` values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "0341ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeName_list = find_placeName(soup_list)\n",
    "placeTags_list = find_placeTags(soup_list)\n",
    "numPeopleVisited_list = find_numPeopleVisited(soup_list)\n",
    "numPeopleWant_list = find_numPeopleWant(soup_list)\n",
    "placeDesc_list = find_placeDesc(soup_list)\n",
    "placeEditors_list = find_placeEditors(soup_list)\n",
    "placeRelated_list = find_placeRelatedLists(soup_list)\n",
    "placeRelatedPlaces_list = find_placeRelatedPlaces(soup_list)\n",
    "placeShortDesc_list = find_placeShortDesc(soup_list)\n",
    "placeNearby_list = find_placeNearby(soup_list)\n",
    "placeAdress_list = find_placeAdress(soup_list)\n",
    "placeAlt_list = find_placeAlt_placeLong(soup_list)[0]\n",
    "placeLong_list = find_placeAlt_placeLong(soup_list)[1]\n",
    "placePubDate_list = find_placePubDate(soup_list)\n",
    "placeRelated_list = find_placeRelatedLists(soup_list)\n",
    "placeURL_list = find_placeURL(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d726bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "\n",
    "df = pd.DataFrame({'placeName':placeName_list, 'placeTags':placeTags_list, 'numPeopleVisited':numPeopleVisited_list,\n",
    "                  'numPeopleWant':numPeopleWant_list, 'placeDesc':placeDesc_list, 'placeShortDesc':placeShortDesc_list,\n",
    "                  'placeNearby':placeNearby_list, 'placeAdress':placeAdress_list, 'placeAlt':placeAlt_list,\n",
    "                   'placeLong':placeLong_list, 'placeEditors':placeEditors_list, 'placePubDate': placePubDate_list,\n",
    "                  'placeRelated':placeRelated_list, 'placeRelatedPlaces':placeRelatedPlaces_list, 'placeURL':placeURL_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14cbd5",
   "metadata": {},
   "source": [
    "* Finally, we save the `df` as a `.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05213138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users\\iadig\\Desktop\\hw3_tsv\\complete_places.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1c14f-b571-47b5-bf63-5c67487b5825",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f3c849-11f9-480f-9381-0e3337ca2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "webdata = pd.read_csv(r\"C:/Users\\stefa\\Desktop\\complete_places.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c27991-6e35-492a-85a3-20c2a4545140",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25d05a-d1e0-4417-bbab-3a406b991f74",
   "metadata": {},
   "source": [
    "* Now we want to remove stopwords, remove punctuation and stem the long description of each place.\n",
    "* We can find stopwords with ntlk package and punctuation with string package. For the stemming we use SnowballStemmer from ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62cb9fb8-bac9-4db6-a76e-755f21303b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = string.punctuation + \"’\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3479e8-0217-4be7-9a85-51cff824aba6",
   "metadata": {},
   "source": [
    "* So we create a function that given the data and the name of a variable, will give a new column in the dataset with the pre-processing operations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04f59d2-ac7c-4fce-95f0-43646f36b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proces(data,name_column):\n",
    "    var = data[name_column]\n",
    "    data['list_words_'+name_column] = var.apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in (stop_words)]))\n",
    "    data['list_words_'+name_column] = data['list_words_'+name_column].apply(lambda x: ' '.join(([(w.translate(str.maketrans('', '', punctuations))) for w in x.split()])))\n",
    "    data['list_words_'+name_column] = data['list_words_'+name_column].apply(lambda x: [stemmer.stem(y) for y in x.split()])\n",
    "    return data['list_words_'+name_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a889ce-457a-4abb-b687-a91ed36341e7",
   "metadata": {},
   "source": [
    "* Pre-processing placeDesc and obtaining list_words_placeDesc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf90fc0-0a5b-4a18-811e-d25ec3009a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [first, new, york, citi, subway, built, oper, ...\n",
       "1       [open, 1839, highgat, one, london, infam, ceme...\n",
       "2       [ornat, 19thcenturi, paint, roof, cobbl, floor...\n",
       "3       [locat, jetti, san, francisco, bay, wave, orga...\n",
       "4       [2004, parisian, polic, assign, train, exercis...\n",
       "                              ...                        \n",
       "7195    [villag, north, carolina, outer, bank, road, s...\n",
       "7196    [eli, cathedr, date, far, back, seventh, centu...\n",
       "7197    [eeri, overrun, lush, greeneri, abandon, theme...\n",
       "7198    [washington, dcs, residenti, neighborhood, typ...\n",
       "7199    [towner, drive, along, dull, stretch, road, ta...\n",
       "Name: list_words_placeDesc, Length: 7200, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_proces(webdata,\"placeDesc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441da571-3270-42da-879e-9086c02b4ae1",
   "metadata": {},
   "source": [
    "## 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67f8de-40f9-4993-93dd-46e89d6eeedf",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a032d0d-3f09-4818-9b49-9c7368cb8e9d",
   "metadata": {},
   "source": [
    "* Now we want to create a vocabulary that links every word in all the long description to an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a3829-190c-4aaa-ba25-4dddd4bb95cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uniqueWord = [];\n",
    "for i in range(webdata.shape[0]):\n",
    "    for j in webdata.list_words_placeDesc[i]:\n",
    "        if j not in uniqueWord:\n",
    "            uniqueWord.append(j)\n",
    "vocabulary={}\n",
    "index_id=[]\n",
    "for i in range(len(uniqueWord)):\n",
    "    vocabulary[uniqueWord[i]]=i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe7464-35a8-455b-a241-da61c81a6842",
   "metadata": {},
   "source": [
    "### Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb69a0-fca0-49cd-96b7-95656d74ccce",
   "metadata": {},
   "source": [
    "* We create the Inverted index that is a dictionary that gives us, for each word ID, the documents in which it is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a29c911-5cb3-4e91-aad5-5297a03ce7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58793/58793 [17:05<00:00, 57.34it/s]\n"
     ]
    }
   ],
   "source": [
    "inverted_idx = {}\n",
    "for word, item_id in tqdm(vocabulary.items()):\n",
    "    inverted_idx[item_id] = list(webdata[webdata.list_words_placeDesc.apply(lambda row: word in row)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383b11a-1d86-48f5-806f-3bbbbf6aa060",
   "metadata": {},
   "source": [
    "## 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ac549-1ad2-46f7-8ded-caef7209420b",
   "metadata": {},
   "source": [
    "* This is a function to pre-processing easily the queries input in the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa01a4d3-b9d8-4126-89c1-e4120ea66993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proces_query(query):\n",
    "    stem_query = ' '.join(([(w.translate(str.maketrans('', '', punctuations))) for w in query.split()]))\n",
    "    stem_query = ' '.join([w for w in stem_query.split() if w.lower() not in (stop_words)])\n",
    "    stem_query = [stemmer.stem(y) for y in stem_query.split()]\n",
    "    return stem_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5912a509-a0fc-4d99-9a85-d52cecf02a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(query):\n",
    "    quer = pre_proces_query(query)\n",
    "    results =[set(inverted_idx[item]) for item in [vocabulary[word] for word in quer]]\n",
    "    first = results[0]\n",
    "    for other in results[1:]:\n",
    "        first = first.intersection(other)\n",
    "    docID = list(reduce(lambda x,y: x.intersection(y), results))\n",
    "    return webdata.loc[docID][[\"placeName\",\"placeDesc\",\"placeURL\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ce9f-c748-4b0b-aab8-ed83d1fa5ce6",
   "metadata": {},
   "source": [
    "* Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3461e564-7598-4f95-b5d3-2a0d564303c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str='new york'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "34d3dd97-25c2-40bd-9e80-5dd7af5599b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City Hall Station</td>\n",
       "      <td>The first New York City subway was built and o...</td>\n",
       "      <td>https://www.atlasobscura.com/places/city-hall-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>The Berkeley Pit</td>\n",
       "      <td>Copper from this former open-pit mine helped t...</td>\n",
       "      <td>https://www.atlasobscura.com/places/berkeley-pit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>The American Geographical Society Library</td>\n",
       "      <td>Within the campus of the University of Wiscons...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>The Girl in Blue</td>\n",
       "      <td>It was Christmas Eve in 1933 in Willoughby, Oh...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-girl-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>Site of New York Slave Market</td>\n",
       "      <td>Now an anonymous condominium, the building sta...</td>\n",
       "      <td>https://www.atlasobscura.com/places/site-of-ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Riot Grrrl Collection at NYU</td>\n",
       "      <td>The Fales Library at New York University has m...</td>\n",
       "      <td>https://www.atlasobscura.com/places/riot-grrrl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>The Sugar House Prison Window</td>\n",
       "      <td>Hidden away on a wall of the New York City Pol...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-sugar-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Ruins of the Cornish Estate</td>\n",
       "      <td>One of the great things to do in New York City...</td>\n",
       "      <td>https://www.atlasobscura.com/places/ruins-of-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>Rokhat Kosher Bakery</td>\n",
       "      <td>In this big, diverse, and sometimes contentiou...</td>\n",
       "      <td>https://www.atlasobscura.com/places/rokhat-kos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>The U.S. Pizza Museum</td>\n",
       "      <td>When it comes to New York and Chicago, pizza i...</td>\n",
       "      <td>https://www.atlasobscura.com/places/us-pizza-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       placeName  \\\n",
       "0                              City Hall Station   \n",
       "2053                            The Berkeley Pit   \n",
       "4106  The American Geographical Society Library    \n",
       "6156                            The Girl in Blue   \n",
       "4108               Site of New York Slave Market   \n",
       "...                                          ...   \n",
       "2013                Riot Grrrl Collection at NYU   \n",
       "4061               The Sugar House Prison Window   \n",
       "2030                 Ruins of the Cornish Estate   \n",
       "6132                        Rokhat Kosher Bakery   \n",
       "2043                       The U.S. Pizza Museum   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "0     The first New York City subway was built and o...   \n",
       "2053  Copper from this former open-pit mine helped t...   \n",
       "4106  Within the campus of the University of Wiscons...   \n",
       "6156  It was Christmas Eve in 1933 in Willoughby, Oh...   \n",
       "4108  Now an anonymous condominium, the building sta...   \n",
       "...                                                 ...   \n",
       "2013  The Fales Library at New York University has m...   \n",
       "4061  Hidden away on a wall of the New York City Pol...   \n",
       "2030  One of the great things to do in New York City...   \n",
       "6132  In this big, diverse, and sometimes contentiou...   \n",
       "2043  When it comes to New York and Chicago, pizza i...   \n",
       "\n",
       "                                               placeURL  \n",
       "0     https://www.atlasobscura.com/places/city-hall-...  \n",
       "2053   https://www.atlasobscura.com/places/berkeley-pit  \n",
       "4106  https://www.atlasobscura.com/places/the-americ...  \n",
       "6156  https://www.atlasobscura.com/places/the-girl-i...  \n",
       "4108  https://www.atlasobscura.com/places/site-of-ne...  \n",
       "...                                                 ...  \n",
       "2013  https://www.atlasobscura.com/places/riot-grrrl...  \n",
       "4061  https://www.atlasobscura.com/places/the-sugar-...  \n",
       "2030  https://www.atlasobscura.com/places/ruins-of-t...  \n",
       "6132  https://www.atlasobscura.com/places/rokhat-kos...  \n",
       "2043  https://www.atlasobscura.com/places/us-pizza-m...  \n",
       "\n",
       "[509 rows x 3 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490dfc3-3357-43dd-891a-66c44c8f3128",
   "metadata": {},
   "source": [
    "## 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032bd3fd-f9e0-4b80-9995-e47f27d1b044",
   "metadata": {},
   "source": [
    "* Now we create a new Inverted index that for each word it gives us the list of documents in which it is contained and the relative tfIdf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b7c27a85-9b4b-4e02-ae6f-7b9b403226a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(input='content', lowercase=False , tokenizer=lambda text:text, max_df=0.5, min_df=2)\n",
    "result=tfidf.fit_transform(webdata.list_words_placeDesc)\n",
    "result_dense= result.todense()\n",
    "results_dense_list =result_dense.tolist()\n",
    "df1 = pd.DataFrame(results_dense_list, index=webdata.index, columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e6042-2776-4ac8-8ce7-46d92528c0b4",
   "metadata": {},
   "source": [
    "* df1 represents the sparse matrix of tfidf scores between words and place descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "86c2fe3a-4697-4876-8744-83aab5d5ebae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "di = {}\n",
    "for word in df1:\n",
    "    li = []\n",
    "    for i,tfidf in enumerate(df1[word]):\n",
    "        if tfidf!=0:\n",
    "            li.append((i,tfidf))\n",
    "    di[vocabulary[word]] = li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936775da-638e-4699-bec5-e05a9af3247a",
   "metadata": {},
   "source": [
    "* Now for example the word \"pizza\" has ID: 11056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "997ccf63-c773-4b29-b2fc-e5963cb1fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11056\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "249cd7aa-fc98-4d86-b893-f09dd10ced51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(371, 0.08691611638664178), (1302, 0.6564147166224242), (1566, 0.049566071487348276), (1648, 0.2051938131032298), (1984, 0.04641030958891165), (1990, 0.07830609468186255), (2043, 0.8679664150183114), (3091, 0.5041686987683972), (3414, 0.06449836499339344), (3480, 0.4537086228766503), (3835, 0.05350701960727532), (3928, 0.07474773850152143), (3945, 0.07705936402997982), (4139, 0.21527139266937365), (4178, 0.043768216626096826), (4526, 0.6009112549476118), (4736, 0.6209293048283294), (4781, 0.19921095652863355), (5854, 0.19180935534391108), (5995, 0.08421189927177535), (6175, 0.4489333629137928), (6205, 0.11802056880216781), (6259, 0.32465652454239824), (6366, 0.1586339946479676), (6396, 0.25595281311528567), (6401, 0.17091094707724913), (6425, 0.09079561068835709), (6663, 0.09493385571651518), (6711, 0.06283019374829278), (6850, 0.07839777762085072), (7094, 0.05680556446438203), (7180, 0.07645982421199359)]\n"
     ]
    }
   ],
   "source": [
    "print(di[11056])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f8247-906a-454e-b4ef-0172e4d8c58c",
   "metadata": {},
   "source": [
    "* These are tuple that represent the documents ID and the corresponding tfidf scores where the word \"pizza\" occcurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d13cf3-fb3c-4c92-ad51-11a809e1abda",
   "metadata": {},
   "source": [
    "## 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7f4c5-c337-40fc-afc4-4b7d1da94e1b",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be600d-7e80-4b59-b736-c1b15443c70d",
   "metadata": {},
   "source": [
    "* Now we create a function that return the cosine similarity between a query and a text (in this case we will use long descriptions), so that we can use it for next points\n",
    "* The cosine similarity formula is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3a25e-8878-4d8a-9691-468eeb4b0adc",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\operatorname{\\textbf{cosine similarity}}(\\mathbf{query}, \\mathbf{document})=\\frac{\\mathbf{query} \\cdot \\mathbf{document}}{|\\mathbf{query} \\| \\mathbf{document}|}=\\frac{\\sum_{i=1}^N (query)_i (document)_i}{\\sqrt{\\sum_{i=1}^N (query)_i^2} \\sqrt{\\sum_{i=1}^N (document)_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "where $query$ is the vector containing the tfidf scores of each word of the processed query based on the query itself and $document$ is the vector containing the tfidf score for a fixed description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83be4c-feed-458e-b7ff-4dc942d9b0f6",
   "metadata": {},
   "source": [
    "* In order to do less iterations and so speed up the function we create two different functions (sim1 and sim2) linked in the function sim_tot. We did this because sim1 can be not repeated for each description se it will be called only once. Instead sim2 is repeated for each description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc83733-fca1-4c79-84b5-cc0f7ae5a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim1(query, data):\n",
    "    query = pre_proces_query(query)\n",
    "    tf_q = []\n",
    "    q = []\n",
    "    for j,word in enumerate(query):\n",
    "        if word not in q:\n",
    "            q.append(word)\n",
    "            tf_q.append(query.count(word)/len(query))\n",
    "    # idf\n",
    "    idf_beta = np.zeros(len(query))\n",
    "    for j,word in enumerate(query):\n",
    "        count = 0\n",
    "        for i in data.index:\n",
    "            if word in data.loc[i]:\n",
    "                count+=1\n",
    "        idf_beta[j] = count\n",
    "    idf = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        if idf_beta[i] != 0:\n",
    "            idf[i] = 1 + np.log(len(data)/idf_beta[i])\n",
    "        else:\n",
    "            idf[i] = 0\n",
    "    # tfidf_query\n",
    "    tfidf_q = tf_q*idf\n",
    "    return q,tfidf_q, idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a628ca6-8fe5-461c-b773-587b16b0dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim2(q, tfidf_q, idf,docu):\n",
    "    tf_d = np.zeros(len(q))\n",
    "    for j,word in enumerate(q):\n",
    "        if len(docu) != 0:\n",
    "            tf_d[j] = docu.count(word)/len(docu)\n",
    "        else:\n",
    "            tf_d[j] = 0\n",
    "    tfidf_d = tf_d*idf\n",
    "    product = np.dot(list(tfidf_q),list(tfidf_d))\n",
    "    quer = np.sqrt(sum(tfidf_q**2))\n",
    "    document =  np.sqrt(sum(tfidf_d**2))\n",
    "    if quer*document != 0:\n",
    "        sim = product/(quer*document)\n",
    "    else:\n",
    "        sim =0  \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c62b8e41-7cf5-4dfc-b73e-bd690e1ca00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_tot(query, data):\n",
    "    q, tfidf_q, idf = sim1(query,data)\n",
    "    a = np.zeros(len(data))\n",
    "    for i,d in enumerate(data):\n",
    "        a[i] = (sim2(q,tfidf_q, idf,d))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac70e09-6210-4389-93dd-cec962be1c3f",
   "metadata": {},
   "source": [
    "* An example of similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "86441d33-90d2-4b37-9ebf-bee3a6006564",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex = \"new york pizza\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d8bc6bb6-b66f-43a9-9f26-79bef949f10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55413448, 0.        , 0.        , ..., 0.        , 0.55413448,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_tot(query_ex, webdata['list_words_placeDesc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afae1e7-466c-469d-a15d-fb0011682848",
   "metadata": {},
   "source": [
    "### Inverted index 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc269f-5523-4e28-ba8b-74e8c8de2d51",
   "metadata": {},
   "source": [
    "* Finally we can create a function that given a query, an integer k and the dataset, will return the first k descrptions where the query is totally conteined in and order by cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beff7c8e-4df6-4b64-8c0d-39a0277548c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index2(query,k,data):\n",
    "    stem_query = pre_proces_query(query)\n",
    "    results =[set(inverted_idx[item]) for item in [vocabulary[word] for word in stem_query]]\n",
    "    first = results[0]\n",
    "    for other in results[1:]:\n",
    "        first = first.intersection(other)\n",
    "    docID = list(reduce(lambda x,y: x.intersection(y), results))\n",
    "    val = sim_tot(query, data.loc[docID]['list_words_placeDesc'])\n",
    "    dictio = dict(sorted(dict(zip(docID, val)).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    a = webdata.loc[dictio.keys()]\n",
    "    a[\"similarity\"] = dictio.values()\n",
    "    return a[[\"placeName\",\"placeDesc\",\"placeURL\",\"similarity\"]].head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b70b3c-1140-4b4a-80f8-8ddd0bbdcffe",
   "metadata": {},
   "source": [
    "* Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "574fe69a-8a82-453d-9caf-caa34ba652c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Hess Triangle</td>\n",
       "      <td>The smallest piece of property in New York Cit...</td>\n",
       "      <td>https://www.atlasobscura.com/places/hess-triangle</td>\n",
       "      <td>0.942809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>300 West 38th Street</td>\n",
       "      <td>Where else but in New York City would you find...</td>\n",
       "      <td>https://www.atlasobscura.com/places/300-west-3...</td>\n",
       "      <td>0.905822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5854</th>\n",
       "      <td>Taste of Persia NYC</td>\n",
       "      <td>In the glitz and polish of Manhattan’s Flatiro...</td>\n",
       "      <td>https://www.atlasobscura.com/places/taste-of-p...</td>\n",
       "      <td>0.852803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>The U.S. Pizza Museum</td>\n",
       "      <td>When it comes to New York and Chicago, pizza i...</td>\n",
       "      <td>https://www.atlasobscura.com/places/us-pizza-m...</td>\n",
       "      <td>0.668437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  placeName  \\\n",
       "371           Hess Triangle   \n",
       "6366   300 West 38th Street   \n",
       "5854    Taste of Persia NYC   \n",
       "2043  The U.S. Pizza Museum   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "371   The smallest piece of property in New York Cit...   \n",
       "6366  Where else but in New York City would you find...   \n",
       "5854  In the glitz and polish of Manhattan’s Flatiro...   \n",
       "2043  When it comes to New York and Chicago, pizza i...   \n",
       "\n",
       "                                               placeURL  similarity  \n",
       "371   https://www.atlasobscura.com/places/hess-triangle    0.942809  \n",
       "6366  https://www.atlasobscura.com/places/300-west-3...    0.905822  \n",
       "5854  https://www.atlasobscura.com/places/taste-of-p...    0.852803  \n",
       "2043  https://www.atlasobscura.com/places/us-pizza-m...    0.668437  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = \"new york and pizza\"\n",
    "inverted_index2(query_2,4,webdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1beb7",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f35165",
   "metadata": {},
   "source": [
    "* `sim2_0` is just a variation of `sim2` from *excercise number 2*. <br>\n",
    "\n",
    "This function also considers the **\"meaninglessness\"** of the words typed into the query by the user. In the previous version of the search engine we **assume** that the query is exclusively composed by words that are known to us (i.e., that are present in our vocabulary), hence the search engine must return a meaningful list of documents. However, this may not necessarily be the case! The user may mistakenly or purposely type mispelled or meaningless words that may not match any of the documents or that may decrease the **similarity** between the query and the documents.<br>\n",
    "\n",
    " Therefore, `sim2_0` is similar to `sim2`, but with the following differences:\n",
    "  * If the `idf` is equal to 0, the word in the query is not present in any of the documents. Therefore, we define `weight` as:\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Weight = \\frac{(length(query) - list(idf).count(0))}{length(query)}\n",
    "  \\end{equation}\n",
    "  \n",
    "   where `list(idf).count(0)` is the number of **unknown** query words (that have `idf` equal to 0). Let's make some examples:\n",
    "   * if **query = \"new unknownWord1 unknownWord2\"**, only one (\"new\") out of 3 words are known, implying that the $weight = 1/3$. \n",
    "   * If **query = \"city hall station\"**, all of the words of the query are known to our vocabulary, hence the $weight = 1$.\n",
    "   * If **query = \"unknownWord1 unknownWord2 unknownWord3\"**, none of the query words are know, hence the $weight = 0$. In this case, the function will return the following message:\n",
    "   \n",
    "   <pre><code>\n",
    "   Sorry, could not find results containing all your search terms.\n",
    "   Make sure that all words are spelled correctly.\n",
    "   Try different keywords.\n",
    "   Try more generic keywords.\n",
    "   \n",
    "   </code></pre>\n",
    "   \n",
    "  * The *cosine similarity* will decrease (or stay invariant) according to the *weight*, for $0 \\leq weight \\leq 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f26ed501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim2_0(q, tfidf_q, idf, docu):\n",
    "    tf_d = np.zeros(len(q))\n",
    "    for j,word in enumerate(q):\n",
    "        if len(docu) != 0:\n",
    "            tf_d[j] = docu.count(word)/len(docu)\n",
    "        else:\n",
    "            tf_d[j] = 0\n",
    "    tfidf_d = tf_d*idf\n",
    "    weight = (len(q)-list(idf).count(0))/len(q)\n",
    "    product = np.dot(list(tfidf_q),list(tfidf_d))\n",
    "    document =  np.sqrt(sum(tfidf_d**2))\n",
    "    if weight != 0:\n",
    "        quer = np.sqrt(sum(tfidf_q**2)/weight)\n",
    "    else:\n",
    "        quer = \"Sorry, could not find results containing all your search terms.\" + \"\\n\" + \"Make sure that all words are spelled correctly.\" + \"\\n\" + \"Try different keywords.\" + \"\\n\" + \"Try more generic keywords.\"\n",
    "    if type(quer) != str:\n",
    "        if quer*document != 0:\n",
    "            sim = product/(quer*document)\n",
    "        else:\n",
    "            sim = 0\n",
    "    else:\n",
    "        sim = quer\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bef4c",
   "metadata": {},
   "source": [
    "* `sim_tot_2` is just the updated version of `sim_tot` which uses `sim2_0` instead of `sim2`, while `sim1` stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "153f8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_tot_2(query, data):\n",
    "    q, tfidf_q, idf = sim1(query,data)\n",
    "    a = np.zeros(len(data))\n",
    "    for i,d in enumerate(data):\n",
    "        a[i] = (sim2_0(q,tfidf_q, idf,d))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207560cd",
   "metadata": {},
   "source": [
    "### Our new score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421828f",
   "metadata": {},
   "source": [
    "* The function `new_score(query, data)` takes as inputs the *query* and the *data* (in our case `webdata`). <br>\n",
    "\n",
    "* In our new score, we not only take into consideration the description `placeDesc`, but also the title `placeName` and the short description `placeShortDesc`.\n",
    "  * We pre-process the title and the short and long descriptions using the `pre_proces` function.\n",
    "  * Then, we calculate the *similarity* between the title and the query using `sim_tot`, the *similarity* between the short description and the query using `sim_tot`, finally we calculate the *similarity* between the long description and the query using `sim_tot_2`. We use the newly defined function `sim_tot_2` only for the long descriptions, as we don't want to excessively penalize the similarity score.\n",
    "  * Finally, the `newscore` is just the *arithmetic mean* of the three similarity scores `sim_title, sim_short & sim_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "026129a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(query, data):\n",
    "    stem_title = pre_proces(data,\"placeName\")\n",
    "    stem_shortDesc = pre_proces(data,\"placeShortDesc\")\n",
    "    stem_longDesc = pre_proces(data,\"placeDesc\")\n",
    "    sim_title = sim_tot(query,stem_title)\n",
    "    sim_short = sim_tot(query,stem_shortDesc)\n",
    "    sim_long = sim_tot_2(query,stem_longDesc)\n",
    "    newscore = (sim_title+sim_short+sim_long)/3\n",
    "    return newscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7e692-e86c-446d-b0c4-5819daaff473",
   "metadata": {},
   "source": [
    "* Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f4dfa10-fe85-4702-b62c-12c525f41695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54415184, 0.        , 0.        , ..., 0.        , 0.21081851,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"elinrv new york edfkjjknkj kh\"\n",
    "new_score(query1,webdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77b10-2eee-4cb5-8eae-99be5b6c4258",
   "metadata": {},
   "source": [
    "### Inverted index 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464ed5e-b5d0-4884-9dee-9afe3bca2e8a",
   "metadata": {},
   "source": [
    "* Now we create a new inverted index using our new score. There will be also the possibility to insert as input an URL and the function will return the corresponding placeName, placeDesc and placeURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2da027b-85d0-42e8-a635-e813f055a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index3(query,k,data):\n",
    "    if query in list(data['placeURL']):\n",
    "        return data.loc[data['placeURL']== query,data.columns[[0,4,14]]]\n",
    "    else:\n",
    "        stem_query = pre_proces_query(query)\n",
    "        results =[set(inverted_idx[item]) for item in [vocabulary[word] for word in stem_query]]\n",
    "        first = results[0]\n",
    "        for other in results[1:]:\n",
    "            first = first.intersection(other)\n",
    "        docID = list(reduce(lambda x,y: x.intersection(y), results))\n",
    "        val = new_score(query, data)\n",
    "        dictio = dict(sorted(dict(zip(docID, val)).items(), key=operator.itemgetter(1),reverse=True))\n",
    "        a = webdata.loc[dictio.keys()]\n",
    "        a[\"new_score\"] = dictio.values()\n",
    "        return a[[\"placeName\",\"placeDesc\",\"placeURL\",\"new_score\"]].head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5311ebe-309d-4dcb-8bc0-da06721c5ed7",
   "metadata": {},
   "source": [
    "* Example with a normal query (using our new_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5a35c69d-286f-41fc-b439-0dc1734b6a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>new_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>Newtown Creek Nature Walk</td>\n",
       "      <td>In one of the last places in New York City whe...</td>\n",
       "      <td>https://www.atlasobscura.com/places/newtown-cr...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>Hamilton Grange</td>\n",
       "      <td>Built in the pastoral expanses of colonial Har...</td>\n",
       "      <td>https://www.atlasobscura.com/places/hamilton-g...</td>\n",
       "      <td>0.996916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City Hall Station</td>\n",
       "      <td>The first New York City subway was built and o...</td>\n",
       "      <td>https://www.atlasobscura.com/places/city-hall-...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Les Jardins de Quatre-Vents</td>\n",
       "      <td>Francis Higginson Cabot loved gardens more tha...</td>\n",
       "      <td>https://www.atlasobscura.com/places/les-jardin...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>Susan B. Anthony's Grave</td>\n",
       "      <td>At the historic and beautiful Mt. Hope Cemeter...</td>\n",
       "      <td>https://www.atlasobscura.com/places/susan-b-an...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>Site of the Beach Pneumatic Subway</td>\n",
       "      <td>Alfred Beach built this pneumatic tunnel to sh...</td>\n",
       "      <td>https://www.atlasobscura.com/places/beach-pneu...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>Venetian Room</td>\n",
       "      <td>This gem of a room at the French Embassy in Ne...</td>\n",
       "      <td>https://www.atlasobscura.com/places/venetian-room</td>\n",
       "      <td>0.660379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>Whirligig Park (Formerly Acid Park)</td>\n",
       "      <td>According to urban legend in Wilson, North Car...</td>\n",
       "      <td>https://www.atlasobscura.com/places/whirligig-...</td>\n",
       "      <td>0.660379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Yonah Schimmel's Knish Bakery</td>\n",
       "      <td>As much of New York’s old Lower East Side disa...</td>\n",
       "      <td>https://www.atlasobscura.com/places/yonah-schi...</td>\n",
       "      <td>0.656535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>The Sugar House Prison Window</td>\n",
       "      <td>Hidden away on a wall of the New York City Pol...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-sugar-...</td>\n",
       "      <td>0.656535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                placeName  \\\n",
       "3744            Newtown Creek Nature Walk   \n",
       "1416                      Hamilton Grange   \n",
       "0                       City Hall Station   \n",
       "2202          Les Jardins de Quatre-Vents   \n",
       "7039             Susan B. Anthony's Grave   \n",
       "5942   Site of the Beach Pneumatic Subway   \n",
       "4057                        Venetian Room   \n",
       "1217  Whirligig Park (Formerly Acid Park)   \n",
       "752         Yonah Schimmel's Knish Bakery   \n",
       "4061        The Sugar House Prison Window   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "3744  In one of the last places in New York City whe...   \n",
       "1416  Built in the pastoral expanses of colonial Har...   \n",
       "0     The first New York City subway was built and o...   \n",
       "2202  Francis Higginson Cabot loved gardens more tha...   \n",
       "7039  At the historic and beautiful Mt. Hope Cemeter...   \n",
       "5942  Alfred Beach built this pneumatic tunnel to sh...   \n",
       "4057  This gem of a room at the French Embassy in Ne...   \n",
       "1217  According to urban legend in Wilson, North Car...   \n",
       "752   As much of New York’s old Lower East Side disa...   \n",
       "4061  Hidden away on a wall of the New York City Pol...   \n",
       "\n",
       "                                               placeURL  new_score  \n",
       "3744  https://www.atlasobscura.com/places/newtown-cr...   1.000000  \n",
       "1416  https://www.atlasobscura.com/places/hamilton-g...   0.996916  \n",
       "0     https://www.atlasobscura.com/places/city-hall-...   0.666667  \n",
       "2202  https://www.atlasobscura.com/places/les-jardin...   0.666667  \n",
       "7039  https://www.atlasobscura.com/places/susan-b-an...   0.666667  \n",
       "5942  https://www.atlasobscura.com/places/beach-pneu...   0.666667  \n",
       "4057  https://www.atlasobscura.com/places/venetian-room   0.660379  \n",
       "1217  https://www.atlasobscura.com/places/whirligig-...   0.660379  \n",
       "752   https://www.atlasobscura.com/places/yonah-schi...   0.656535  \n",
       "4061  https://www.atlasobscura.com/places/the-sugar-...   0.656535  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ind3 = \"new york\"\n",
    "inverted_index3(query_ind3,10,webdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a1c24-a487-4846-a479-b00a86e708ef",
   "metadata": {},
   "source": [
    "* Example with the same query but using the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bfcf2962-6e90-4ef1-bb38-03b370eaa67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Roosevelt Island Smallpox Hospital Ruins</td>\n",
       "      <td>Few diseases have had a greater impact on the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/roosevelt-...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>Coney Island Creek</td>\n",
       "      <td>At nearly 10 acres, Coney Island Creek is a si...</td>\n",
       "      <td>https://www.atlasobscura.com/places/coney-isla...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166</th>\n",
       "      <td>Castle Craig</td>\n",
       "      <td>This lone tower pokes above the rocks within a...</td>\n",
       "      <td>https://www.atlasobscura.com/places/castle-craig</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Assassin's End</td>\n",
       "      <td>On the evening of April 14th, 1865, famed acto...</td>\n",
       "      <td>https://www.atlasobscura.com/places/assassins-end</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>Jackson Sanatorium</td>\n",
       "      <td>In its lifetime, the Jackson Sanatorium has ha...</td>\n",
       "      <td>https://www.atlasobscura.com/places/jackson-sa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>The Underground Home</td>\n",
       "      <td>In the 1960s, the Cuban Missile Crisis posed a...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-underg...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Yayoi Kusama Firefly Infinity Mirror Room</td>\n",
       "      <td>The 89-year-old Japanese avant-garde artist Ya...</td>\n",
       "      <td>https://www.atlasobscura.com/places/yayoi-kusa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Track 61</td>\n",
       "      <td>Unlike other “abandoned” train stations in the...</td>\n",
       "      <td>https://www.atlasobscura.com/places/track-61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The SeaGlass Carousel</td>\n",
       "      <td>The SeaGlass Carousel in New York City’s Batte...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-seagla...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>The 'Ghostbusters' Firehouse</td>\n",
       "      <td>As you walk down North Moore Street in lower M...</td>\n",
       "      <td>https://www.atlasobscura.com/places/ghostbuste...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      placeName  \\\n",
       "18     Roosevelt Island Smallpox Hospital Ruins   \n",
       "6162                         Coney Island Creek   \n",
       "6166                               Castle Craig   \n",
       "4128                             Assassin's End   \n",
       "4134                         Jackson Sanatorium   \n",
       "2087                       The Underground Home   \n",
       "57    Yayoi Kusama Firefly Infinity Mirror Room   \n",
       "69                                     Track 61   \n",
       "71                        The SeaGlass Carousel   \n",
       "75                 The 'Ghostbusters' Firehouse   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "18    Few diseases have had a greater impact on the ...   \n",
       "6162  At nearly 10 acres, Coney Island Creek is a si...   \n",
       "6166  This lone tower pokes above the rocks within a...   \n",
       "4128  On the evening of April 14th, 1865, famed acto...   \n",
       "4134  In its lifetime, the Jackson Sanatorium has ha...   \n",
       "2087  In the 1960s, the Cuban Missile Crisis posed a...   \n",
       "57    The 89-year-old Japanese avant-garde artist Ya...   \n",
       "69    Unlike other “abandoned” train stations in the...   \n",
       "71    The SeaGlass Carousel in New York City’s Batte...   \n",
       "75    As you walk down North Moore Street in lower M...   \n",
       "\n",
       "                                               placeURL  similarity  \n",
       "18    https://www.atlasobscura.com/places/roosevelt-...         1.0  \n",
       "6162  https://www.atlasobscura.com/places/coney-isla...         1.0  \n",
       "6166   https://www.atlasobscura.com/places/castle-craig         1.0  \n",
       "4128  https://www.atlasobscura.com/places/assassins-end         1.0  \n",
       "4134  https://www.atlasobscura.com/places/jackson-sa...         1.0  \n",
       "2087  https://www.atlasobscura.com/places/the-underg...         1.0  \n",
       "57    https://www.atlasobscura.com/places/yayoi-kusa...         1.0  \n",
       "69         https://www.atlasobscura.com/places/track-61         1.0  \n",
       "71    https://www.atlasobscura.com/places/the-seagla...         1.0  \n",
       "75    https://www.atlasobscura.com/places/ghostbuste...         1.0  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index2(query_ind3,10,webdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad44e6d-e6cd-4e4a-a45c-7dfc38d556a7",
   "metadata": {},
   "source": [
    "* As we can see our new score will be more precise ordering the place because it will analyze also the placeName and the placeShortdesc for each place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd6d24-674d-4ae2-bd1b-91d982ae7682",
   "metadata": {},
   "source": [
    "* As we said, if we give in input an URL it will return the right row of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "739c4b8e-8e04-4b15-9d83-1c6cb582231a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Roosevelt Island Smallpox Hospital Ruins</td>\n",
       "      <td>Few diseases have had a greater impact on the ...</td>\n",
       "      <td>https://www.atlasobscura.com/places/roosevelt-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   placeName  \\\n",
       "18  Roosevelt Island Smallpox Hospital Ruins   \n",
       "\n",
       "                                            placeDesc  \\\n",
       "18  Few diseases have had a greater impact on the ...   \n",
       "\n",
       "                                             placeURL  \n",
       "18  https://www.atlasobscura.com/places/roosevelt-...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index3(\"https://www.atlasobscura.com/places/roosevelt-island-smallpox-hospital-ruins\",10,webdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b2340-5e6a-4eee-a51a-48fa7375b21b",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae1402-813b-4d0f-a3e4-5a5fe4deea78",
   "metadata": {},
   "source": [
    "* Now let visualize on an earth map the locations of the k place more related (with our new_score) to a specific query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80db1625-d71b-4de7-b62a-60291954d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"new york\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d9e55-e2e1-4c5f-8d45-d6ff4aa1da5a",
   "metadata": {},
   "source": [
    "* In order to not change our previusly function output, we do a merge between the original dataframe and the one with only placeName, placeDesc and placeURL of first k ordered places. This is because we will need latitude and longitude for each of these places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6089cb17-3849-4819-9cc0-852506219fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inverted_index3(query_4,10,webdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "954f10cb-a83d-42f8-8979-278046693765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_k = pd.merge(df,webdata,on=[\"placeName\",\"placeDesc\",\"placeURL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ff70c-fb27-462a-98f2-c78a4b06f65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_new=folium.Map()\n",
    "list_tags = list(df_k['placeTags'])\n",
    "list_lon = list(df_k['placeLong'])\n",
    "list_lat = list(df_k['placeAlt'])\n",
    "for i in range(len(list_lon)):\n",
    "    map_new.add_child(folium.Marker(location=[list_lat[i],list_lon[i]], popup=list_tags[i],icon=folium.Icon(color='green')))\n",
    "map_new  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad22756-755b-4a99-8b1a-d58f2047a37d",
   "metadata": {},
   "source": [
    "* This is the final representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd1388-9bda-43c8-8f4c-4ff64b72d0b3",
   "metadata": {},
   "source": [
    "<img src=\"map_point_4_0.jpg\"  width=80% height=80%> <img src=\"map_point_4.jpg\"  width=80% height=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c02edf-afd9-40fc-9d6b-729279b56835",
   "metadata": {},
   "source": [
    "* These are two images of the same map with different zoom levels for the example query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73421fee-0929-4f4d-a3bf-4dcb570400b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7 Teoretical question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda7123-069e-4f0f-adcf-635b76a8d6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "An imaginary university wants to create a ranking list of the applicants for positions to study the Master of Data Science there.Students are then admitted based on how well they perform on these exams, they should be ranked in the list based on their average points in descending order, if two students have the same average punctuation, they should be sorted in ascending order using their first and last names.\n",
    "University will give you the students' information in 'ApplicantsInfo.txt' and you should provide them with the ranking list in another .txt file and name it as 'RankingList.txt' ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e0ff4-615e-48d6-b338-0cad77f4ca60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8668d3e-a46e-4040-8213-9bac7dea12da",
   "metadata": {},
   "source": [
    "Try solving the problem mentioned above using three different sorting algorithms (do not use any MapReduce algorithm). (Note: Built-in Python functions (like .mean, .sort, etc.) are not allowed to be used. You must implement the algorithms from scratch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ea6cb-241f-4bf4-923d-b32b1bc1cc11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Auxiliary function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6310547-2f7f-417f-b004-2df391c6fd58",
   "metadata": {},
   "source": [
    "#### read ApplicantsInfo.txt: students and averages lists\n",
    "The below function reads line by line the file \"ApplicantsInfo.txt\" and creates two lists the firt containg name and surname of the students and in the other one stores the exams'average, these lists will be the starting point for the sorting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4e5577-54d0-4127-8d10-3143dd1ae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplicantsInfo(): \n",
    " with open(r\"C:\\Users\\wiwis\\OneDrive\\Desktop\\ADM\\3°HM\\ApplicantsInfo.txt\",'r') as f:\n",
    "   n,m=list(map(int,f.readline().split()))\n",
    "        \n",
    "   current_line_list=f.readline().split()\n",
    "   averages_list=[\"{:.2f}\".format(sum(list(map(int,current_line_list[2:])))/m)]\n",
    "   students_list=[current_line_list[0]+' '+current_line_list[1]]\n",
    "    \n",
    "   for i in range(1,n): \n",
    "     current_line_list=f.readline().split()\n",
    "     averages_list.append(\"{:.2f}\".format(sum(list(map(int,current_line_list[2:])))/m))   \n",
    "     students_list.append(current_line_list[0]+' '+current_line_list[1])  \n",
    "   \n",
    " return n,m,averages_list,students_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761a1992-6f1f-45dd-9557-4aeeccfd5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m,averages_list_unsorted,students_list_unsorted=ApplicantsInfo() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6907ce-edf4-4c37-84b0-0c64eea2fc94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### write RankingList.txt \n",
    "The function RankingList creates the file 'RankingList.txt' using sorted lists that will be returned by  the sorting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518d6e6b-1633-44ab-94d6-686214dc478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankingList(averages_list, students_list, sorting):\n",
    "  with open(r\"C:\\Users\\wiwis\\OneDrive\\Desktop\\ADM\\3°HM\\RankingList\"+sorting+\".txt\",'w') as g:\n",
    "          g.write(students_list[0]+' '+str(averages_list[0])+'\\n') \n",
    "          for i in range(1,n-1):                                          \n",
    "              g.write(students_list[i]+' '+str(averages_list[i])+'\\n')  \n",
    "          g.write(students_list[n-1]+' '+str(averages_list[n-1]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e80542-cd6b-446b-8cc7-97278cdba6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sorting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469dd75-1871-4d89-97c7-63e1b3aeedcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sorting by BINARY SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad1d6e5-755e-49ca-b3ac-24bc91be8d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binarySort(averages_list_unsorted,students_list_unsorted,n,m):  \n",
    "   averages_list=[averages_list_unsorted[0]]\n",
    "   students_list=[students_list_unsorted[0]]\n",
    "  \n",
    "\n",
    "   for i in range(1,n):\n",
    "     current_average=averages_list_unsorted[i]\n",
    "     current_student=students_list_unsorted[i]\n",
    "     \n",
    "     \n",
    "    \n",
    "     if current_average >= averages_list[0]:\n",
    "        if current_average > averages_list[0]:\n",
    "          averages_list.insert(0, current_average )\n",
    "          students_list.insert(0,current_student)\n",
    "        else:\n",
    "          j=0\n",
    "          current_length=len(averages_list)  \n",
    "          while j+1<current_length and averages_list[j] == current_average and current_student > students_list[j]     :\n",
    "                j+=1\n",
    "          averages_list.insert(j, current_average )\n",
    "          students_list.insert(j,current_student)   \n",
    "         \n",
    "     elif current_average <= averages_list[-1]:\n",
    "        if current_average < averages_list[-1]:\n",
    "          averages_list.append(current_average )\n",
    "          students_list.append(current_student)\n",
    "        else:\n",
    "          j=len(averages_list)\n",
    "          while j-1>=0 and averages_list[j-1] == current_average and  current_student < students_list[j-1]    :\n",
    "                j+=(-1)\n",
    "          averages_list.insert(j, current_average)\n",
    "          students_list.insert(j,current_student)\n",
    "     else:    \n",
    "        first = 0\n",
    "        last = len(averages_list)-1\n",
    "        found=False\n",
    "        while first<last and not(found):\n",
    "              j = (first + last)//2  #midpoint\n",
    "              if current_average == averages_list[j]:\n",
    "                found=True\n",
    "                while j-1>=0 and averages_list[j] == current_average and current_student < students_list[j]    :\n",
    "                      j+=(-1)\n",
    "                j+=1        \n",
    "                current_length=len(averages_list)         \n",
    "                while j+1<current_length and averages_list[j] == current_average and current_student > students_list[j]    :\n",
    "                      j+=1        \n",
    "              else:  \n",
    "                if current_average > averages_list[j]:\n",
    "                   last = j-1\n",
    "                else:\n",
    "                   first = j+1\n",
    "        \n",
    "        if averages_list[j]> current_average:\n",
    "           current_length=len(averages_list)         \n",
    "           while j+1<current_length and  current_average == averages_list[j+1] and  current_student > students_list[j+1]  :\n",
    "                      j+=1 \n",
    "           averages_list.insert(j+1, current_average )\n",
    "           students_list.insert(j+1,current_student)\n",
    "        elif averages_list[j]<current_average:\n",
    "           while  j-1>=0 and averages_list[j-1]==current_average and current_student < students_list[j-1]:\n",
    "                      j+=(-1)\n",
    "           averages_list.insert(j, current_average )\n",
    "           students_list.insert(j,current_student)\n",
    "        else:\n",
    "           averages_list.insert(j, current_average )\n",
    "           students_list.insert(j,current_student)\n",
    "            \n",
    "   return  averages_list, students_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd4065e-37c6-4772-bad0-34aa90fd5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_list_sort, students_list_sort = binarySort(averages_list_unsorted,students_list_unsorted,n,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b898c90-4263-4351-8c16-ae5ed4acd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "RankingList(averages_list_sort , students_list_sort, 'Binarysearch')\n",
    "#print('The algorithm perform in '+ str((b-a)[0])+' hours '+ str((b-a)[1])+' mins '+ str(((b-a)[2]+60)%60)+' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47351d-2a0f-4b7a-b267-8ff962b2256a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sorting by MERGESORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afbb290-b2e0-4b7a-aec6-69ece574a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(averages_list, students_list, l, m, r):  # let n_merge=(r-l)\n",
    "    n1 = m - l + 1        \n",
    "    n2 = r - m               \n",
    "    \n",
    "    #2 op(addition)\n",
    "    \n",
    "    L_av= [0]*n1\n",
    "    L_st= [0]*n1 \n",
    "    R_av = [0]*n2 \n",
    "    R_st = [0]*n2 \n",
    "    \n",
    "    #2*n_merge op(allocation)\n",
    "    \n",
    "    for i in range(0, n1):\n",
    "        L_av[i] = averages_list[l + i]\n",
    "        L_st[i] = students_list[l + i]\n",
    "    for j in range(0, n2):\n",
    "        R_av[j] = averages_list[m + 1 + j]\n",
    "        R_st[j] = students_list[m + 1 + j]\n",
    "     \n",
    "    \n",
    "    #2*n_merge op(allocation)\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = l\n",
    "    \n",
    "    #3 op(allocation)\n",
    "    \n",
    "    while i < n1 and j < n2:\n",
    "        if L_av[i] > R_av[j] or (L_av[i] == R_av[j] and L_st[i]<=R_st[j]):\n",
    "            averages_list[k] = L_av[i]\n",
    "            students_list[k] = L_st[i]\n",
    "            i += 1     \n",
    "        else:        \n",
    "            averages_list[k] = R_av[j]\n",
    "            students_list[k] = R_st[j]\n",
    "            j += 1            \n",
    "        k += 1\n",
    "    \n",
    "    # 2*(k) op(boolean check) (while and if) + 2*(k) op(allocation) where min(n1,n2)<= k <=n\n",
    "    \n",
    "    while i < n1:    \n",
    "        averages_list[k] = L_av[i]\n",
    "        students_list[k] = L_st[i]\n",
    "        i += 1\n",
    "        k += 1\n",
    "\n",
    "    while j < n2:    \n",
    "        averages_list[k] = R_av[j]\n",
    "        students_list[k] = R_st[j]\n",
    "        j += 1\n",
    "        k += 1\n",
    "        \n",
    "    # (n_merge-k+1) op(boolean check) + (n_merge-k) op(allocation)\n",
    "    \n",
    "    return averages_list,students_list\n",
    "\n",
    "def mergeSort(averages_list,students_list, l, r): #let n=(r-l)\n",
    "    if l < r:\n",
    "        m=(l+r)//2   \n",
    "       \n",
    "        mergeSort(averages_list, students_list, l, m)    #T(m-l)~T(n/2)\n",
    "        mergeSort(averages_list, students_list, m+1, r)  #T(r-m+1)~T(n/2)\n",
    "        merge(averages_list, students_list, l, m, r)     #T_merge(n)\n",
    "    return averages_list,students_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1f12a7-02d5-4aaa-8482-c1f525006db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_list_sort =  averages_list_unsorted.copy()\n",
    "students_list_sort =  students_list_unsorted.copy()\n",
    "  \n",
    "averages_list_sort,students_list_sort = mergeSort(averages_list_sort,students_list_sort, 0, len(averages_list_sort)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1733f6-7e5a-4979-bda6-394fb3270c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RankingList(averages_list_sort , students_list_sort, 'Merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb72ed-6df0-44ba-9b36-8bb33c11c77d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sorting by QUICKSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaeb6b59-3b2e-46f3-a2a8-3b783941ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(averages_list,students_list, l, r):    #let n_partition=(r-l)                                                                                             \n",
    "                                                                                                   \n",
    "    pivot = averages_list[r]                                                                                                  \n",
    "    i = l - 1  \n",
    "    \n",
    "    #2 op(allocation)\n",
    "    \n",
    "    for j in range(l, r):                                                                                                                                                                                     \n",
    "        if averages_list[j] > pivot or (averages_list[j] == pivot and students_list[j]< students_list[r]):                                                                                                                                                                                     \n",
    "            i = i + 1                                                                                                                                                                                      \n",
    "            (averages_list[i], averages_list[j]) = (averages_list[j], averages_list[i])                                                                                        \n",
    "            (students_list[i], students_list[j]) = (students_list[j], students_list[i])                                                                                                                                                                                    \n",
    "    \n",
    "    # 2*n_partition op(boolean check) +  3*(n_partition-k) op(allocation)     where 0 <= k <=n\n",
    "    \n",
    "    (averages_list[i + 1], averages_list[r]) = (averages_list[r], averages_list[i + 1])                                                                                                                                   \n",
    "    (students_list[i + 1], students_list[r]) = (students_list[r], students_list[i + 1])    \n",
    "    \n",
    "    #2 op(allocation)\n",
    "    \n",
    "    return i + 1                                                                                                                                                                                     \n",
    "                                                                                                  \n",
    "                                                                                                                                                                                      \n",
    "def quickSort(averages_list,students_list, l, r):   #let n=(r-l)\n",
    "    if l < r:                                                                                                                                                                                     \n",
    "        pi = partition(averages_list,students_list, l, r)        #T_partition(n)                                                                                                                                                                             \n",
    "        quickSort(averages_list,students_list, l, pi - 1)        #T((pi-1)-l)\n",
    "        quickSort(averages_list,students_list, pi + 1,r)         #T(r-(pi+1))                                                                                                                                                                            \n",
    "                                                                                                                                                                                                                                              \n",
    "    return averages_list,students_list                                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3294d9c1-9921-4cf4-b89e-193dd3f79e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_list_sort =  averages_list_unsorted.copy()\n",
    "students_list_sort =  students_list_unsorted.copy()\n",
    "\n",
    "averages_list_sort,students_list_sort = quickSort(averages_list_sort,students_list_sort, 0, len(students_list_sort)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd33d90-0aad-442f-8607-16f118c56e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "RankingList(averages_list_sort , students_list_sort, 'Quick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9900dd4-430c-40b9-b49d-5b5770af5fd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c96ca9-e3dd-40da-b627-694d0ffa28de",
   "metadata": {},
   "source": [
    " What is the time complexity of each algorithm you have used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c0eca-c1f9-4393-896b-54d882fb36c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "* ###  complexity **mergeSort** : \n",
    "<br>\n",
    "Adding up the number of all operations in the code of the procedure \"merge\" we have:\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T_{\\text{merge}}(n_{\\text{merge}})=2+2n_{\\text{merge}} + 2n_{\\text{merge}} +3 + 2k +2k + (n_{\\text{merge}}-k+1) + (n_{\\text{merge}}-k)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "where  $\\quad 0 \\leq min\\{n_1,n_2\\} \\leq k \\leq n \\quad$ with $n_1$ and $n_2$ as defined in the code, so\n",
    "<br><br> \n",
    "$ \n",
    "\\begin{align*}\n",
    "T_{\\text{merge}}(n_{\\text{merge}}) = 5+6n_{\\text{merge}} +2k \\qquad \\implies \\qquad 5+6n_{\\text{merge}} \\leq T_{\\text{merge}}(n_{\\text{merge}}) \\leq 5+8n_{\\text{merge}} \\quad (\\ast)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "the global complexity $T(n)$ of the algorithm as shown in the code is:\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T(n) \\quad = \\quad 2 T\\left( \\frac{n}{2} \\right) +T_{\\text{merge}}(n) \\quad = \\quad 2\\left( 2 T\\left( \\frac{n}{2} \\cdot \\frac{1}{2}\\right)+T_{\\text{merge}}\\left( \\frac{n}{2} \\right) \\right)  +T_{\\text{merge}}\\left( n \\right)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "= \\quad \\dots \\quad = \\quad 2^iT\\left( \\frac{n}{2^i} \\right) +\\sum_{j=1}^i 2^{j-1} T_{\\text{merge}}\\left( \\frac{n}{2^{j-1}} \\right)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\implies T(n) \\quad = \\quad 2^{\\log_2(n)} T\\left( \\frac{n}{2^{\\log_2(n)}} \\right) +\\underbrace{\\sum_{j=1}^{\\log_2(n)} 2^{j-1}T_{\\text{merge}}\\left( \\frac{n}{2^{j-1}} \\right)}_{A}\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "focusing on $A$ ...\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^{\\log_2(n)} 2^{j-1}T_{\\text{merge}}\\left( \\frac{n}{2^{j-1}} \\right) \\quad \\leq \\quad \\sum_{j=1}^{\\log_2(n)} 2^{j-1}\\left( 5 + 8 \\cdot \\frac{n}{2^{j-1}} \\right) \\quad = \\quad 8n \\log_2(n) +5\\sum_{j=1}^{\\log_2(n)} 2^{j-1}\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^{\\log_2(n)} 2^{j-1} \\quad = \\quad \\sum_{j=0}^{\\log_2(n)-1} 2^{j} \\quad = \\quad \\frac{1-2^{\\left( \\log_2(n)-1 \\right) +1}}{1-2} \\quad = \\quad n -1\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "in conclusion\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\implies T(n) \\quad \\leq \\quad nT(1) +  8n \\log_2(n) +5(n -1) \\qquad \\text{ and similarly using $(\\ast)$} \\quad T(n) \\quad \\geq \\quad nT(1) +  6n \\log_2(n) +5(n -1)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "Since $\\quad T(1)=1 \\quad $ results $\\quad T(n)= \\Theta\\left( 6n \\log_2(n)+6n \\right) \\quad $ or equivalent $\\quad T(n)= \\Theta\\left( n \\log_2(n)\\right) \\quad $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad373886-b414-45e2-b914-a19f5aa9183d",
   "metadata": {},
   "source": [
    "* ###  complexity **quickSort** :\n",
    "\n",
    "<br>\n",
    "Adding up the number of all operations in the code of the procedure \"partition\" we have:\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T_{\\text{partition}}(n_{\\text{partition}})= 2+2 n_{\\text{partition}} +3\\left(n_{\\text{partition}} -k \\right) +2\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "where  $\\quad 0  \\leq k \\leq n \\quad$, so\n",
    "<br><br> \n",
    "$ \n",
    "\\begin{align*}\n",
    "T_{\\text{partition}}(n_{\\text{partition}}) = 4+5n_{\\text{partition}} -3k \\qquad \\implies \\qquad 4+2n_{\\text{merge}} \\leq T_{\\text{partition}}(n_{\\text{partition}}) \\leq 4+5n_{\\text{merge}} \\quad (\\ast)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "The global complexity $T(n)$ of the algorithm as shown in the code is:\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T(n) \\quad = \\quad  T\\left( (pi-1)-l \\right) + T\\left( r-(pi+1) \\right)  +T_{\\text{partition}}(n) \n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "with $l$,$r$ and $pi$ as defined in the code. <br>\n",
    "Now we can interpret the computational complexity of the algorithm as a random variable $T(n,pi)$ dependent on the position of the pivot $pi$, whose probability of occupying any position $j$ after the partition procedure can be assumed to be $P(pi=j)=1/n$; with these assumptions we can approximate the average complexity with its expected value.\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T(n) \\quad = \\quad \\mathbb{E}(T(n,pi)) \\quad = \\sum_{j=0}^{n-1} P(pi=j) T(n,pi=j) \\quad  \\implies T(n) \\quad = \\quad \\sum_{j=0}^{n-1} \\frac{1}{n} \\big( T \\left( j \\right) + T\\left( n-(j+1) \\right)  +T_{\\text{partition}}(n) \\big)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "we observe that $T \\left( j \\right)$ and $T\\left( n-(j+1) \\right)$ generate the same terms, so\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T(n) \\quad = \\quad T_{\\text{partition}}(n) + \\frac{2}{n}\\sum_{j=0}^{n-1} T \\left( j \\right) \\qquad\n",
    "\\implies \\quad nT(n) \\quad = \\quad nT_{\\text{partition}}(n) + 2\\sum_{j=0}^{n-1} T \\left( j \\right) \\qquad (a)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "similarly\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "(n-1)T(n-1) \\quad = \\quad (n-1)T_{\\text{partition}}(n-1) + 2\\sum_{j=0}^{n-2} T \\left( j \\right) \\qquad  (b)\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "subtracting $(b)$ from $(a)$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "nT(n) -(n-1)T(n-1) \\quad = \\quad nT_{\\text{partition}}(n) - (n-1)T_{\\text{partition}}(n-1) + 2 T \\left( n-1 \\right) \n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\leq \\quad n(4+5n) - (n-1)(4+5(n-1)) + 2 T \\left( n-1 \\right) \\quad = \\quad \\dots \\quad = \\quad 10n +9 + 2 T \\left( n-1 \\right) \\qquad \\implies \\quad nT(n) \\leq (n+1)T(n-1) + 10n +9\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\implies \\frac{T(n)}{(n+1)} \\leq \\frac{T(n-1)}{n} + \\frac{10}{(n+1)} +\\underbrace{\\frac{9}{n(n+1)}}_{\\text{negligible because $\\sum \\frac{1}{n^2}$ converges}} \\leq \\left(  \\frac{T(n-2)}{n-1} + \\frac{10}{n} \\right) + \\frac{10}{(n-1)} \\quad \\leq \\quad \\dots \\quad \\leq \\quad \\frac{T(1)}{2} + 10 \\underbrace{ \\sum_{j=1}^n \\frac{1}{j+1} }_{B}\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "it is possible to show that for n tending to infinity ( reference 1) $B$ tends to $\\quad \\ln n + \\gamma \\quad $ where $\\quad \\gamma=0.577 \\dots \\quad$ is the Eulero-Mascheroni's costant, since $T(1)$ is equal to 1  this two condition implies \n",
    "<br><br>\n",
    "$\n",
    "\\begin{align*}\n",
    "T(n) \\quad \\leq \\quad \\frac{n+1}{2} + 10(n+1) \\ln (n) +(n+1)\\gamma \\qquad \\implies \\qquad T(n) =O(n \\log(n))\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "To find a lowerbound of the complexity we know that $\\quad T(n)=\\Omega(T_{\\text{best}}(n)) \\quad $ and since the best instance for the algoritm correspond to the case when for each recursive call of the algorithm the pivot occurs always in the midpoint position, we have under this assumptions, $\\quad T_{\\text{best}}(n)= 2  T_{\\text{best}}(n/2) + T_{\\text{partition}}(n) \\quad $ but this setting is equivalent to the previous one regarding the mergeSort, because $\\quad T_{\\text{partition}}(n)=\\Theta(n)=T_{\\text{merge}}(n) \\quad$, so in conclusion $\\quad T_{\\text{best}}(n)= T_{\\text{mergeSort}}(n)=\\Theta(n \\log(n))\\quad $ and $\\quad T(n)=\\Theta(n \\log(n)) \\quad$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20854cd-9933-4abf-bd7d-8a137c9eec09",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "1. [https://mate.unipv.it/gilardi/WEBGG/PSPDF/eulero-masch.pdf](https://mate.unipv.it/gilardi/WEBGG/PSPDF/eulero-masch.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c71bf-23f8-4260-92f2-b972e028d463",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a77be-2e46-44e5-9688-a4ade3453493",
   "metadata": {},
   "source": [
    "Evaluate the time taken for each of your implementations to answer the query stored in the ApplicantsInfo.txt file and visualize them.\n",
    "<br>\n",
    "We estimate the time taken by the three algorithms to compute the scores' averages and sort the students in the order demand from the question 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f2c4d0-cf2b-46cd-ba0d-c416b3288bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_binary=datetime.now()\n",
    "\n",
    "for i in range(100):\n",
    " averages_list_sort, students_list_sort = binarySort(averages_list_unsorted,students_list_unsorted,n,m)\n",
    "\n",
    "end_time_binary=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbee24d-3a18-4bb6-9e7a-953feefc3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_merge=datetime.now()\n",
    "\n",
    "for i in range(100):\n",
    "  averages_list_sort =  averages_list_unsorted.copy()\n",
    "  students_list_sort =  students_list_unsorted.copy()\n",
    "  \n",
    "  averages_list_sort,students_list_sort = mergeSort(averages_list_sort,students_list_sort, 0, len(averages_list_sort)-1)\n",
    "  \n",
    "end_time_merge=datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71b86eeb-cc1a-4fcf-a618-4760222a52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_quick=datetime.now()\n",
    "\n",
    "for i in range(100):\n",
    "  averages_list_sort =  averages_list_unsorted.copy()\n",
    "  students_list_sort =  students_list_unsorted.copy()\n",
    "    \n",
    "  averages_list_sort,students_list_sort = quickSort(averages_list_sort,students_list_sort, 0, len(students_list_sort)-1)\n",
    "\n",
    "end_time_quick=datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24474a4-5f40-4ea8-a3b1-0a1797d9d0ac",
   "metadata": {},
   "source": [
    "Here the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "589c7312-9268-4f96-8117-241e11257cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_binary=str(end_time_binary-start_time_binary).split(':')\n",
    "time_merge=str(end_time_merge-start_time_merge).split(':')\n",
    "time_quick =str(end_time_quick-start_time_quick).split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e61a872d-6b12-4923-9afa-311a4a57daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the algorithm binarySort takes 08 mins and 04 sec\n",
      "the algorithm mergeSort takes 01 mins and 44 sec\n",
      "the algorithm quickSort takes 00 mins and 50 sec\n"
     ]
    }
   ],
   "source": [
    "print('the algorithm binarySort takes '+time_binary[1]+' mins and '+time_binary[2][:2]+' sec')\n",
    "print('the algorithm mergeSort takes '+time_merge[1]+' mins and '+time_merge[2][:2]+' sec')\n",
    "print('the algorithm quickSort takes '+time_quick[1]+' mins and '+time_quick[2][:2]+' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c795a-ea0c-4d8a-bccf-e7a19bedf2db",
   "metadata": {},
   "source": [
    "## 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b3b18-047b-4d5f-9e7b-1a41284b8969",
   "metadata": {},
   "source": [
    "What is the most optimal algorithm, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de3555-cc73-485c-a406-dab99c5becd7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee10a15-a6c4-48fe-8879-4ab387775a96",
   "metadata": {},
   "source": [
    "The quickSort algorithm takes about half as long as the mergeSort to run, it is therefore the fastest to satisfy the requirement of ordering the students' scores, the binarySort on the other hand observing the conclusions in 7. 3 is very slow, compared to the other two-algorithms it has, however, the merit of not using a recursive procedure in fact such an implementation, after making a few attempts, turns out to be inefficient when the input list is sorted opposite to the desired one, if so in fact the excessive number of recursive calls leads to the forced kernel interruption (at least using computers with ordinary characteristics), in conclusion since the list \"averages_list_unsorted\" given as input does not present this problem, it is legitimate to suppose that this list of numeric values of the students' averages does not present an order,  in the specific case of question 7 the quickSort algorithm turns out to be the optimal one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
